{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T20:39:45.581622600Z",
     "start_time": "2024-01-16T20:39:45.577333600Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "import bitsandbytes\n",
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_json('../data/training_data.json', orient='records')\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', load_in_8bit=True, torch_dtype=torch.float16)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Function to generate token embeddings from text part of batch\n",
    "def _preprocess_batch(batch: Dict[str, List]):  \n",
    "    model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')    \n",
    "    model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n",
    "    return model_inputs\n",
    "\n",
    "_preprocessing_function = partial(_preprocess_batch)\n",
    "\n",
    "# apply the preprocessing function to each batch in the dataset\n",
    "encoded_small_dataset = small_dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"response\", \"prompt\", \"answer\"],\n",
    ")\n",
    "processed_dataset = encoded_small_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n",
    "\n",
    "# splitting dataset\n",
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "print(split_dataset)\n",
    "\n",
    "# takes a list of samples from a Dataset and collate them into a batch, as a dictionary of PyTorch tensors.\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        model = model, tokenizer=tokenizer, max_length=MAX_LENGTH, pad_to_multiple_of=8, padding='max_length')\n",
    "\n",
    "LORA_R = 256 # 512\n",
    "LORA_ALPHA = 512 # 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "                 r = LORA_R, # the dimension of the low-rank matrices\n",
    "                 lora_alpha = LORA_ALPHA, # scaling factor for the weight matrices\n",
    "                 lora_dropout = LORA_DROPOUT, # dropout probability of the LoRA layers\n",
    "                 bias=\"none\",\n",
    "                 task_type=\"CAUSAL_LM\",\n",
    "                 target_modules=[\"query_key_value\"],\n",
    ")\n",
    "\n",
    "# Prepare int-8 model for training - utility function that prepares a PyTorch model for int8 quantization training. <https://huggingface.co/docs/peft/task_guides/int8-asr>\n",
    "model = prepare_model_for_int8_training(model)\n",
    "# initialize the model with the LoRA framework\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-4  \n",
    "MODEL_SAVE_FOLDER_NAME = \"dolly-3b-lora\"\n",
    "training_args = TrainingArguments(\n",
    "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
    "                    overwrite_output_dir=True,\n",
    "                    fp16=True, #converts to float precision 16 using bitsandbytes\n",
    "                    per_device_train_batch_size=1,\n",
    "                    per_device_eval_batch_size=1,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    logging_strategy=\"epoch\",\n",
    "                    evaluation_strategy=\"epoch\",\n",
    "                    save_strategy=\"epoch\",\n",
    ")\n",
    "# training the model \n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=test,\n",
    "        data_collator=data_collator,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "# only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\n",
    "trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
    "# save the full model and the training arguments\n",
    "trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
    "trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "808c69e19f36f409"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
